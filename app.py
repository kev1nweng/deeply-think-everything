#!/usr/bin/env python3

import os
import time
import openai
import configparser
import re
import warnings
import json
from typing import Generator
from rich.markdown import Markdown
from rich.console import Console
from rich.syntax import Syntax
from wcwidth import wcswidth
from pylatexenc.latex2text import LatexNodes2Text
from prompt_toolkit import prompt

warnings.filterwarnings("ignore", category=SyntaxWarning)

# è¯»å–é…ç½®æ–‡ä»¶
script_dir = os.path.dirname(os.path.realpath(__file__))
config = configparser.ConfigParser()

# è¯»å–å„ç§é…ç½®
try:
    config.read(f"{script_dir}/config.ini")
except Exception as e:
    print("è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥:", e)

try:
    with open(f"{script_dir}/prompts.json", "r") as f:
        prompts = json.load(f)
        if (not prompts["think_prompt"]) or not (prompts["answer_prompt"]):
            raise Exception("æç¤ºè¯æ–‡ä»¶æ ¼å¼é”™è¯¯")
except Exception as e:
    print("è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥:", e)
    exit(1)

try:
    class Config:
        API_KEY = config.get("dte", "api_key")
        API_BASE = config.get("dte", "api_url")
        MODEL_NAME = config.get("dte", "model_name")
except Exception as e:
    print("è¯»å–é…ç½®å‚æ•°å¤±è´¥:", e)
    exit(1)

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å’ŒRichæ§åˆ¶å°
client = openai.OpenAI(api_key=Config.API_KEY, base_url=Config.API_BASE)
console = Console(record=True)


def dynamic_separator(title: str) -> str:
    """ç”ŸæˆåŠ¨æ€å±…ä¸­åˆ†éš”çº¿
    æ ¹æ®ç»ˆç«¯å®½åº¦è‡ªåŠ¨è°ƒæ•´åˆ†éš”çº¿é•¿åº¦ï¼Œç¡®ä¿æ ‡é¢˜å±…ä¸­æ˜¾ç¤º
    wcswidthç”¨äºå‡†ç¡®è®¡ç®—åŒ…å«ä¸­æ–‡ç­‰å®½å­—ç¬¦çš„æ˜¾ç¤ºå®½åº¦
    """

    term_width = console.width
    title = f" {title} "
    title_width = wcswidth(title)
    available_width = term_width - title_width
    if available_width < 2:
        return "=" * term_width
    sep_len = available_width // 2
    return f"[dim]{'='*sep_len}[/][bold]{title}[/][dim]{'='*(available_width - sep_len)}[/]"


def _handle_block_latex(match: re.Match) -> str:
    """å¤„ç†å—çº§LaTeXå…¬å¼
    å°†LaTeXæ•°å­¦å…¬å¼è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œå¹¶æ·»åŠ ç¾è§‚çš„è¾¹æ¡†
    """

    formula = match.group(1) or match.group(2)
    converted = LatexNodes2Text().latex_to_text(formula).strip()
    return f"\nâ”Œ {' LaTeX ':-^8} â”\n{converted}\nâ”” {'-'*8} â”˜\n"


def _process_latex_in_text(text: str) -> str:
    r"""å¤„ç†éä»£ç å—ä¸­çš„LaTeXå…¬å¼
    1. å¤„ç†è¡Œå†…å…¬å¼: $formula$ -> â”†formulaâ”†
    2. å¤„ç†å—çº§å…¬å¼: $$formula$$ æˆ– \[formula\] -> å¸¦æ¡†æ˜¾ç¤º
    3. é¿å…å¤„ç†å·²è½¬ä¹‰çš„LaTeXæ ‡è®°
    """

    # å¤„ç†è¡Œå†…å…¬å¼ï¼ˆæ’é™¤è½¬ä¹‰$çš„æƒ…å†µï¼‰
    text = re.sub(
        r"(?<!\\)\$(.*?)(?<!\\)\$",
        lambda m: f"[dim]â”†[/] {LatexNodes2Text().latex_to_text(m.group(1)).strip()} [dim]â”†[/]",
        text,
        flags=re.DOTALL,
    )
    # å¤„ç†å—çº§å…¬å¼ï¼ˆæ’é™¤è½¬ä¹‰æƒ…å†µï¼‰
    return re.sub(
        r"(?<!\\)\$\$([\s\S]*?)(?<!\\)\$\$|(?<!\\)\\\[([\s\S]*?)(?<!\\)\\\]",
        _handle_block_latex,
        text,
        flags=re.DOTALL,
    )


def preprocess_latex(content: str) -> str:
    """æ”¹è¿›åçš„LaTeXé¢„å¤„ç†
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å†…å®¹ï¼Œç¡®ä¿ä»£ç å—ä¸­çš„LaTeXç¬¦å·ä¸è¢«å¤„ç†
    å¥‡æ•°ç´¢å¼•çš„éƒ¨åˆ†æ˜¯ä»£ç å—ï¼Œç›´æ¥ä¿ç•™
    å¶æ•°ç´¢å¼•çš„éƒ¨åˆ†æ˜¯æ™®é€šæ–‡æœ¬ï¼Œéœ€è¦å¤„ç†LaTeX
    """

    parts = re.split(r"(```[\s\S]*?```)", content)
    processed = []
    for i, part in enumerate(parts):
        if i % 2 == 1:
            processed.append(part)
            continue
        processed.append(_process_latex_in_text(part))
    return "".join(processed)


def render_stream_markdown(content: str):
    """æ”¹è¿›åçš„æµå¼Markdownæ¸²æŸ“å¤„ç†å™¨
    1. é¢„å¤„ç†æ‰€æœ‰LaTeXå…¬å¼
    2. ç‰¹æ®Šå¤„ç†ä»£ç å—ï¼Œæ”¯æŒè¯­æ³•é«˜äº®
    3. æ™®é€šæ–‡æœ¬ä½¿ç”¨Markdownæ¸²æŸ“
    """

    content = preprocess_latex(content)
    in_code_block = False
    code_buffer = []
    current_lang = "text"

    lines = content.split("\n")
    for line in lines:
        if line.strip().startswith("```"):
            if in_code_block:
                console.print(
                    Syntax(
                        "\n".join(code_buffer),
                        current_lang,
                        theme="monokai",
                        line_numbers=False,
                    )
                )
                code_buffer = []
                in_code_block = False
            else:
                lang = line.strip()[3:].strip()
                current_lang = lang if lang else "text"
                in_code_block = True
            continue
        if in_code_block:
            code_buffer.append(line)
        else:
            console.print(Markdown(line))


def stream_think_process(
    conversation_history: list, question: str
) -> Generator[str, None, None]:
    """è·å–æ·±åº¦æ€è€ƒåˆ†æï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    1. ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å¼•å¯¼AIè¿›è¡Œæ·±åº¦åˆ†æ
    2. ä¿æŒè¾ƒä½temperature(0.3)ä»¥ç¡®ä¿è¾“å‡ºçš„è¿è´¯æ€§å’Œé€»è¾‘æ€§
    3. é™åˆ¶æœ€å¤§tokenä»¥é¿å…å“åº”è¿‡é•¿
    è¿”å›: (åˆ†æç»“æœ, è€—æ—¶)
    """

    system_prompt = prompts["think_prompt"]

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(conversation_history)
    messages.append({"role": "user", "content": question})

    response = client.chat.completions.create(
        model=Config.MODEL_NAME,
        messages=messages,
        temperature=0.3,
        max_tokens=1500,
        stream=True,
    )

    for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


def stream_final_answer(
    conversation_history: list, analysis: str, question: str
) -> Generator[str, None, None]:
    """æµå¼ç”Ÿæˆæœ€ç»ˆå›ç­”
    1. å°†å†å²å¯¹è¯ã€åˆ†æç»“æœå’Œå½“å‰é—®é¢˜æ•´åˆ
    2. ä½¿ç”¨è¾ƒé«˜temperature(0.7)å…è®¸æ›´æœ‰åˆ›é€ æ€§çš„å›ç­”
    3. æµå¼è¾“å‡ºä»¥æä¾›æ›´å¥½çš„äº¤äº’ä½“éªŒ
    yields: å›ç­”å†…å®¹çš„ç‰‡æ®µ
    """

    system_prompt = prompts["answer_prompt"]

    messages = [{"role": "system", "content": system_prompt + "\n" + analysis}]
    messages.extend(conversation_history)
    messages.append({"role": "user", "content": f"åŸºäºåˆ†æå†…å®¹ï¼Œè¯·å›ç­”ï¼š{question}"})

    response = client.chat.completions.create(
        model=Config.MODEL_NAME,
        messages=messages,
        stream=True,
        temperature=0.7,
        max_tokens=4096,
    )

    for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


def format_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ—¶é—´"""
    return f"{seconds:.2f}s" if seconds >= 1 else f"{seconds*1000:.0f}ms"


if __name__ == "__main__":
    console.print(
        "[bold magenta]ğŸ‘‹ æ¬¢è¿æ¥åˆ° DeeplyThinkEverythingï¼\nè¾“å…¥ @new å¯ä»¥æ¸…é™¤ä¸Šä¸‹æ–‡å¹¶å¼€å¯æ–°å¯¹è¯ã€‚[/]"
    )
    conversation_history = []

    try:
        while True:
            try:
                question = prompt("\n>> ", multiline=False).strip()
                if not question:
                    continue

                if question.lower() == "@new":
                    conversation_history.clear()
                    console.clear()
                    console.print("\nğŸŒŸ [cyan]ä¸Šä¸‹æ–‡å·²é‡ç½®ï¼Œå¼€å¯æ–°å¯¹è¯[/]")
                    continue

                console.print("\nğŸ¤” [yellow]æ­£åœ¨æ·±åº¦æ€è€ƒ...[/]")
                console.print(f"\n{dynamic_separator('æ€è€ƒè¿‡ç¨‹åˆ†æ')}\n")

                # æµå¼å¤„ç†æ€è€ƒè¿‡ç¨‹
                start_time = time.time()
                analysis_content = []
                buffer = ""

                for chunk in stream_think_process(conversation_history, question):
                    buffer += chunk
                    processed_part = _process_latex_in_text(buffer)
                    console.print(processed_part, end="")
                    analysis_content.append(processed_part)
                    buffer = ""

                # å¤„ç†å‰©ä½™å†…å®¹
                if buffer:
                    processed_part = _process_latex_in_text(buffer)
                    console.print(processed_part, end="")
                    analysis_content.append(processed_part)

                think_time = time.time() - start_time
                analysis = "".join(analysis_content)
                console.print(f"\nğŸ” [dim]æ·±åº¦æ€è€ƒè€—æ—¶: {format_time(think_time)}[/]")

                console.print(f"\n{dynamic_separator('æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ')}\n")
                answer_buffer = []
                start_time = time.time()
                full_content = ""

                try:
                    for chunk in stream_final_answer(
                        conversation_history, analysis, question
                    ):
                        full_content += chunk
                        answer_buffer.append(chunk)

                    render_stream_markdown(full_content)

                except Exception as e:
                    if full_content:
                        render_stream_markdown(full_content)
                    console.print(f"\nâš ï¸ [red]ç”Ÿæˆä¸­æ–­: {str(e)}[/]")
                finally:
                    if full_content:
                        conversation_history.extend(
                            [
                                {"role": "user", "content": question},
                                {"role": "assistant", "content": full_content},
                            ]
                        )

                    total_time = time.time() - start_time
                    console.print("\n" + "=" * console.width)
                    console.print(
                        f"âœ… [green]ç”Ÿæˆå®Œæˆ[/] | [dim]è€—æ—¶: {format_time(total_time)}[/] | "
                        f"[dim]ä¸Šä¸‹æ–‡é•¿åº¦: {len(conversation_history)//2}è½®[/]"
                    )

            except KeyboardInterrupt:
                console.print(
                    "\nğŸ›‘ [red]æ“ä½œå·²å–æ¶ˆï¼Œåœ¨ 1s å†…å†æ¬¡æŒ‰ä¸‹ Ctrl+C é€€å‡ºç¨‹åº[/]"
                )
                time.sleep(1)

    except KeyboardInterrupt:
        console.print("\nğŸ‘‹ å†è§ï¼")
        exit()
    except Exception as e:
        console.print(f"\nâŒ [red]å¤„ç†é”™è¯¯: {str(e)}[/]")
        conversation_history.clear()
