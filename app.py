import time
import openai
from typing import Generator
from rich.markdown import Markdown
from rich.console import Console
from rich.syntax import Syntax
from wcwidth import wcswidth  # ç”¨äºå‡†ç¡®è®¡ç®—å­—ç¬¦æ˜¾ç¤ºå®½åº¦
import configparser


# è¯»å–é…ç½®æ–‡ä»¶
config = configparser.ConfigParser()
try:
    config.read("config.ini")
except Exception as e:
    print("è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥:", e)


# é…ç½®å‚æ•°
class Config:
    API_KEY = (
        config.get("dte", "api_key")
    )
    API_BASE = config.get("dte", "api_url")
    MODEL_NAME = config.get("dte", "model_name")


# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å’ŒRichæ§åˆ¶å°
client = openai.OpenAI(api_key=Config.API_KEY, base_url=Config.API_BASE)
console = Console(record=True)


def dynamic_separator(title: str) -> str:
    """ç”ŸæˆåŠ¨æ€å±…ä¸­åˆ†éš”çº¿"""
    term_width = console.width
    title = f" {title} "  # ä¸ºæ ‡é¢˜æ·»åŠ ä¸¤ä¾§ç©ºæ ¼
    title_width = wcswidth(title)

    # è®¡ç®—å¯ç”¨ç©ºé—´
    available_width = term_width - title_width
    if available_width < 2:
        return "=" * term_width

    # è®¡ç®—ä¸¤ä¾§ç­‰å·æ•°é‡
    sep_len = available_width // 2
    return f"[dim]{'='*sep_len}[/][bold]{title}[/][dim]{'='*(available_width - sep_len)}[/]"


def render_stream_markdown(content: str):
    """æµå¼Markdownæ¸²æŸ“å¤„ç†å™¨"""
    in_code_block = False
    code_buffer = []
    current_lang = "text"

    lines = content.split("\n")
    for line in lines:
        # å¤„ç†ä»£ç å—æ ‡è®°
        if line.strip().startswith("```"):
            if in_code_block:
                # æ¸²æŸ“ä»£ç å—
                console.print(
                    Syntax(
                        "\n".join(code_buffer),
                        current_lang,
                        theme="monokai",
                        line_numbers=False,
                    )
                )
                code_buffer = []
                in_code_block = False
            else:
                # è§£æä»£ç è¯­è¨€
                lang = line.strip()[3:].strip()
                current_lang = lang if lang else "text"
                in_code_block = True
            continue

        if in_code_block:
            code_buffer.append(line)
        else:
            # æ¸²æŸ“æ™®é€šMarkdownå†…å®¹
            console.print(Markdown(line))

    # å¤„ç†æœªé—­åˆçš„ä»£ç å—
    if in_code_block and code_buffer:
        console.print(
            Syntax(
                "\n".join(code_buffer),
                current_lang,
                theme="monokai",
                line_numbers=False,
            )
        )


def get_think_process(conversation_history: list, question: str) -> tuple[str, float]:
    """è·å–æ·±åº¦æ€è€ƒåˆ†æï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ·±åº¦æ€è€ƒè¾…åŠ©AIï¼Œåˆ†ææ—¶éœ€è€ƒè™‘å¯¹è¯å†å²ï¼š
    - è¯†åˆ«å½“å‰é—®é¢˜ä¸å†å²è®°å½•çš„å…³è”æ€§
    - åˆ†æä¸Šä¸‹æ–‡æ¼”è¿›ä¸­çš„çŸ¥è¯†ç§¯ç´¯
    - æ³¨æ„ç”¨æˆ·å¯èƒ½å­˜åœ¨çš„è¿½é—®æˆ–æ¦‚å¿µå»¶ä¼¸
    ä¿æŒåŸæœ‰åˆ†ææ¡†æ¶ï¼Œä½†éœ€ç‰¹åˆ«æ ‡æ³¨ä¸å†å²å¯¹è¯çš„å…³è”ç‚¹ã€‚è¾“å‡ºä¿æŒmarkdownä»£ç å—æ ¼å¼ã€‚"""

    start_time = time.time()

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(conversation_history)
    messages.append({"role": "user", "content": question})

    response = client.chat.completions.create(
        model=Config.MODEL_NAME,
        messages=messages,
        temperature=0.3,
        max_tokens=1500,
    )

    return response.choices[0].message.content, time.time() - start_time


def stream_final_answer(
    conversation_history: list, analysis: str, question: str
) -> Generator[str, None, None]:
    """æµå¼ç”Ÿæˆæœ€ç»ˆå›ç­”ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    system_prompt = """ä½ æ˜¯å›ç­”ç”ŸæˆAIï¼Œéœ€ç»¼åˆä»¥ä¸‹è¦ç´ ï¼š
    1. å½“å‰æ·±åº¦æ€è€ƒåˆ†æ
    2. å®Œæ•´å¯¹è¯å†å²ä¸Šä¸‹æ–‡
    3. ä¿æŒå›ç­”è¿è´¯æ€§å’ŒçŸ¥è¯†é€’è¿›æ€§
    ç‰¹åˆ«æ³¨æ„å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - å½“é—®é¢˜æ¶‰åŠå…ˆå‰æ¦‚å¿µæ—¶è‡ªåŠ¨å…³è”è§£é‡Š
    - å¯¹é‡å¤æé—®æ™ºèƒ½è¯†åˆ«å¹¶å·®å¼‚åŒ–å“åº”
    - ä¸Šä¸‹æ–‡çŸ›ç›¾æ—¶çš„æ¾„æ¸…å¤„ç†"""

    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(conversation_history)
    messages.append({"role": "assistant", "content": analysis})
    messages.append(
        {"role": "user", "content": f"åŸºäºåˆ†æå’Œå†å²å¯¹è¯ï¼Œè¯·å›ç­”ï¼š{question}"}
    )

    response = client.chat.completions.create(
        model=Config.MODEL_NAME,
        messages=messages,
        stream=True,
        temperature=0.7,
        max_tokens=2000,
    )

    for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content


def format_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ—¶é—´"""
    return f"{seconds:.2f}s" if seconds >= 1 else f"{seconds*1000:.0f}ms"


if __name__ == "__main__":
    console.print(
        "[bold magenta]ğŸ‘‹ æ¬¢è¿æ¥åˆ° DeeplyThinkEverythingï¼\nè¾“å…¥ @new å¯ä»¥æ¸…é™¤ä¸Šä¸‹æ–‡å¹¶å¼€å¯æ–°å¯¹è¯ã€‚[/]"
    )
    conversation_history = []

    try:
        while True:
            try:
                question = input("\n>> ").strip()
                if not question:
                    continue

                # ä¸Šä¸‹æ–‡é‡ç½®æŒ‡ä»¤
                if question.lower() == "@new":
                    conversation_history.clear()
                    console.clear()
                    console.print("\nğŸŒŸ [cyan]ä¸Šä¸‹æ–‡å·²é‡ç½®ï¼Œå¼€å¯æ–°å¯¹è¯[/]")
                    continue

                console.print("\nğŸ¤” [yellow]æ­£åœ¨æ·±åº¦æ€è€ƒ...[/]")

                # é˜¶æ®µ1ï¼šå¸¦ä¸Šä¸‹æ–‡çš„æ€è€ƒåˆ†æ
                analysis, think_time = get_think_process(conversation_history, question)

                # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                console.print(f"\n{dynamic_separator('æ€è€ƒè¿‡ç¨‹åˆ†æ')}\n")
                console.print(Markdown(analysis))
                console.print(f"\nğŸ” [dim]æ·±åº¦æ€è€ƒè€—æ—¶: {format_time(think_time)}[/]")

                # é˜¶æ®µ2ï¼šå¸¦ä¸Šä¸‹æ–‡çš„æµå¼å›ç­”
                console.print(f"\n{dynamic_separator('æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ')}\n")
                answer_buffer = []
                start_time = time.time()
                full_content = ""

                try:
                    # æµå¼æ¥æ”¶ä½†ç»Ÿä¸€æ¸²æŸ“
                    for chunk in stream_final_answer(
                        conversation_history, analysis, question
                    ):
                        full_content += chunk
                        answer_buffer.append(chunk)

                    # ç»Ÿä¸€æ¸²æŸ“Markdown
                    render_stream_markdown(full_content)

                except Exception as e:
                    # å°è¯•æ¸²æŸ“å·²æ¥æ”¶å†…å®¹
                    if full_content:
                        render_stream_markdown(full_content)
                    console.print(f"\nâš ï¸ [red]ç”Ÿæˆä¸­æ–­: {str(e)}[/]")
                finally:
                    # æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
                    if full_content:
                        conversation_history.extend(
                            [
                                {"role": "user", "content": question},
                                {"role": "assistant", "content": full_content},
                            ]
                        )

                    # æ€§èƒ½ç»Ÿè®¡
                    total_time = time.time() - start_time
                    console.print("\n" + "=" * console.width)
                    console.print(
                        f"âœ… [green]ç”Ÿæˆå®Œæˆ[/] | [dim]æ€»è€—æ—¶: {format_time(total_time)}[/] | "
                        f"[dim]ä¸Šä¸‹æ–‡é•¿åº¦: {len(conversation_history)//2}è½®[/]"
                    )

            except KeyboardInterrupt:
                console.print("\nğŸ›‘ [red]æ“ä½œå·²å–æ¶ˆï¼Œå†æ¬¡æŒ‰ä¸‹ Ctrl+C é€€å‡ºç¨‹åº[/]")
                time.sleep(1)  # ç­‰å¾…ç¬¬äºŒæ¬¡ä¸­æ–­

    except KeyboardInterrupt:
        console.print("\nğŸ‘‹ å†è§ï¼")
        exit()
    except Exception as e:
        console.print(f"\nâŒ [red]å¤„ç†é”™è¯¯: {str(e)}[/]")
        conversation_history.clear()
